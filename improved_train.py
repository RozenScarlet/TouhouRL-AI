# improved_train.py
"""
æ”¹è¿›çš„è®­ç»ƒè„šæœ¬ - é›†æˆç›‘æ§å’Œè¯Šæ–­åŠŸèƒ½
"""

import numpy as np
import torch
import time
import os
import sys
from training_monitor import TrainingMonitor

from env import ImprovedTouhouEnv
from PPO import ImprovedPPOAgent

def train_with_monitoring(model_path=None, force_restart=False):
    """å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•°"""
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = TrainingMonitor()
    
    # åˆ›å»ºproject_player/modelsç›®å½•
    models_dir = "models"
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = ImprovedTouhouEnv()
    env.debug_rewards = True  # å¯ç”¨å¥–åŠ±è°ƒè¯•
    agent = ImprovedPPOAgent(n_actions=9)

    n_episodes = 10000
    save_interval = 10
    monitor_interval = 5  # æ¯5è½®ç”Ÿæˆä¸€æ¬¡ç›‘æ§æŠ¥å‘Š
    best_reward = float('-inf')
    start_episode = 0

    # æ¨¡å‹åŠ è½½é€»è¾‘
    if not force_restart and model_path and os.path.exists(model_path):
        try:
            agent.load(model_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
            # ä»æ–‡ä»¶åæå–episodeæ•°
            if "ppo_" in model_path:
                try:
                    start_episode = int(model_path.split("ppo_")[1].split(".")[0])
                    print(f"ä»Episode {start_episode}ç»§ç»­è®­ç»ƒ")
                except:
                    pass
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ")

    print(f"\nå¼€å§‹è®­ç»ƒï¼Œç›®æ ‡è½®æ•°: {n_episodes}")
    print(f"ä¿å­˜é—´éš”: {save_interval} episodes")
    print(f"ç›‘æ§é—´éš”: {monitor_interval} episodes")
    print("-" * 60)

    # è®­ç»ƒå¾ªç¯
    for episode in range(start_episode, n_episodes):
        episode_start_time = time.time()
        
        print(f"\nğŸ® å¼€å§‹Episode {episode + 1}")
        
        state = env.reset()
        total_reward = 0
        steps = 0
        policy_loss = None
        value_loss = None
        
        # Episodeæ•°æ®æ”¶é›†
        episode_bullet_distances = []
        episode_rewards_breakdown = []
        
        done = False
        while not done:
            # PPOè·å–åŠ¨ä½œå’Œä»·å€¼
            action, log_prob, value = agent.get_action(state)
            next_state, reward, done, info = env.step(action)

            # æ”¶é›†ç›‘æ§æ•°æ®
            if 'min_bullet_dist' in info:
                episode_bullet_distances.append(info['min_bullet_dist'])
                monitor.log_step({'bullet_distance': info['min_bullet_dist']})
            
            if hasattr(env, 'last_reward_breakdown'):
                episode_rewards_breakdown.append(env.last_reward_breakdown.copy())

            # å­˜å‚¨è½¬æ¢åˆ°PPOç¼“å†²åŒº
            agent.store_transition(state, action, reward, log_prob, value, done)

            state = next_state
            total_reward += reward
            steps += 1

            # å®æ—¶æ˜¾ç¤ºçŠ¶æ€ï¼ˆå‡å°‘é¢‘ç‡é¿å…åˆ·å±ï¼‰
            if steps % 100 == 0:
                print(f"\ræ­¥æ•°: {steps} | å¥–åŠ±: {total_reward:.2f} | ç¼“å†²åŒº: {agent.buffer.size()}", end="")

            if done:
                episode_end_time = time.time()
                survival_time = episode_end_time - episode_start_time
                
                print(f"\næ¸¸æˆç»“æŸï¼ŒåŸå› : {'ç”Ÿå‘½è€—å°½' if info['lives'] == 0 else 'å…¶ä»–åŸå› '}")
                print(f"å­˜æ´»æ—¶é—´: {survival_time:.1f}ç§’")
                
                # ç»“æŸepisodeï¼Œè®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
                agent.finish_episode(next_state if not done else None)
                
                # å°è¯•æ›´æ–°ç½‘ç»œ
                policy_loss, value_loss = agent.update()
                
                # æ·»åŠ episodeå¥–åŠ±ç”¨äºè·Ÿè¸ª
                agent.add_episode_reward(total_reward)
                
                # è®°å½•ç›‘æ§æ•°æ®
                episode_data = {
                    'total_reward': total_reward,
                    'steps': steps,
                    'survival_time': survival_time,
                    'death_reason': 'bullet_collision' if info['lives'] == 0 else 'other'
                }
                
                # æ·»åŠ å¥–åŠ±åˆ†è§£ç»Ÿè®¡
                if episode_rewards_breakdown:
                    avg_breakdown = {}
                    for key in episode_rewards_breakdown[0].keys():
                        avg_breakdown[key] = np.mean([rb[key] for rb in episode_rewards_breakdown])
                    episode_data['reward_breakdown'] = avg_breakdown
                
                monitor.log_episode(episode_data)
                
                state = env.reset()
                break

        # å®šæœŸä¿å­˜å’Œç›‘æ§
        if (episode + 1) % save_interval == 0:
            save_path = os.path.join(models_dir, f"ppo_{episode+1}.pth")
            agent.save(save_path)
            print(f"ğŸ’¾ å·²ä¿å­˜æ¨¡å‹: {save_path}")

        # å®šæœŸç”Ÿæˆç›‘æ§æŠ¥å‘Š
        if (episode + 1) % monitor_interval == 0:
            monitor.print_status(episode + 1)
            report = monitor.generate_report(episode + 1)
            monitor.plot_training_curves()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            if report and report['performance']['avg_reward_last_100'] > best_reward:
                best_reward = report['performance']['avg_reward_last_100']
                best_save_path = os.path.join(models_dir, f"ppo_best_{episode+1}.pth")
                agent.save(best_save_path)
                print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {best_reward:.2f}")

        # æ‰“å°episodeä¿¡æ¯
        print(f"\nğŸ“Š Episode {episode+1} å®Œæˆ:")
        print(f"æ€»æ­¥æ•°: {steps}")
        print(f"æ€»å¥–åŠ±: {total_reward:.2f}")
        if policy_loss is not None:
            print(f"ç­–ç•¥æŸå¤±: {policy_loss:.4f}")
            print(f"ä»·å€¼æŸå¤±: {value_loss:.4f}")
        print(f"å¹³å‡å¥–åŠ±: {agent.get_average_reward():.2f}")
        print(f"åˆ†æ•°: {info['score']}")
        print(f"å‰©ä½™ç”Ÿå‘½: {info['lives']}")
        
        if episode_bullet_distances:
            avg_bullet_dist = np.mean(episode_bullet_distances)
            min_bullet_dist = min(episode_bullet_distances)
            print(f"å¹³å‡å­å¼¹è·ç¦»: {avg_bullet_dist:.1f}")
            print(f"æœ€å°å­å¼¹è·ç¦»: {min_bullet_dist:.1f}")
            
        print("-" * 50)

    # è®­ç»ƒç»“æŸ
    final_save_path = os.path.join(models_dir, "ppo_final.pth")
    agent.save(final_save_path)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    final_report = monitor.generate_report(n_episodes)
    monitor.plot_training_curves(os.path.join(models_dir, "final_training_curves.png"))
    
    env.close()
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_save_path}")
    print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°modelsç›®å½•")
    
    if final_report:
        print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒç»Ÿè®¡:")
        print(f"å¹³å‡å¥–åŠ±: {final_report['performance']['avg_reward_last_100']:.2f}")
        print(f"å¹³å‡å­˜æ´»æ—¶é—´: {final_report['performance']['avg_survival_time']:.1f}ç§’")
        print(f"æœ€é«˜å¥–åŠ±: {final_report['performance']['max_reward']:.2f}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ”¹è¿›çš„ä¸œæ–¹AIè®­ç»ƒè„šæœ¬')
    parser.add_argument('--model', type=str, help='è¦åŠ è½½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--restart', action='store_true', help='å¼ºåˆ¶é‡æ–°å¼€å§‹è®­ç»ƒ')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ æ”¹è¿›çš„ä¸œæ–¹çº¢é­”ä¹¡AIè®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print("ä¸»è¦æ”¹è¿›:")
    print("âœ… ä¼˜åŒ–å¥–åŠ±å‡½æ•° - æ›´å¹³è¡¡çš„å¥–åŠ±æœºåˆ¶")
    print("âœ… æ”¹è¿›æ­»äº¡æƒ©ç½š - åŸºäºå­˜æ´»æ—¶é—´çš„åŠ¨æ€å¥–åŠ±")
    print("âœ… å¢å¼ºå­å¼¹æ£€æµ‹ - å¤šé˜ˆå€¼æ£€æµ‹ç®—æ³•")
    print("âœ… å®æ—¶ç›‘æ§ - è¯¦ç»†çš„è®­ç»ƒçŠ¶æ€ç›‘æ§")
    print("âœ… è°ƒè¯•åŠŸèƒ½ - å¥–åŠ±åˆ†è§£å’Œæ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    train_with_monitoring(model_path=args.model, force_restart=args.restart)
